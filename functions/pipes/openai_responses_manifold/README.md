# OpenAI Responses Manifold

Enables advanced OpenAI features (function calling, web search, visible reasoning summaries, and more) directly in [Open WebUI](https://github.com/open-webui/open-webui).

**Now supports OpenAI‚Äôs GPT-5 family in the API ‚Äî [Learn more](#gpt-5-model-support).**

This project started as an internal tool (200+ hours of optimization and testing) and is now open-sourced as a way to give back to the Open WebUI community.

> ‚ú® Like the manifold? 
> Show your support by sharing it with others and providing feedback through [GitHub Discussions](https://github.com/jrkropp/open-webui-developer-toolkit/discussions).  
> üí° Pull Requests are welcome!


## Contents

* [Setup](#setup)
* [Local Development](#local-development)
* [Features](#features)
* [Advanced Features](#advanced-features)
* [Logging & Diagnostics](#logging--diagnostics)
* [Tested Models](#tested-models)
* [GPT‚Äë5 Model Support](#gpt5-model-support)
* [How It Works (Design Notes)](#how-it-works-design-notes)
* [Troubleshooting / FAQ](#troubleshooting--faq)

## Setup
1. In **Open WebUI ‚ñ∏ Admin Panel ‚ñ∏ Functions**, click **Import from Link**.
   
   <img width="450" alt="image" src="https://github.com/user-attachments/assets/4a5a0355-e0af-4fb8-833e-7d3dfb7f10e3" />

2. Paste one of the following links, depending on which version you want:

   * **Main** (recommended) ‚Äì Stable production build with regular, tested updates:

     ```
     https://github.com/jrkropp/open-webui-developer-toolkit/blob/main/functions/pipes/openai_responses_manifold/openai_responses_manifold.py
     ```

   * **Alpha Preview** ‚Äì Pre-release build with early features, typically 2‚Äì4 weeks ahead of main:

     ```
     https://github.com/jrkropp/open-webui-developer-toolkit/blob/alpha-preview/functions/pipes/openai_responses_manifold/openai_responses_manifold.py
     ```

3. **‚ö†Ô∏è Important: Set the Function ID to `openai_responses`.**
   
   This value is currently hardcoded in the pipe and must match exactly. It will become configurable in a future release.
   
   <img width="800" alt="image" src="https://github.com/user-attachments/assets/ffd3dd72-cf39-43fa-be36-56c6ac41477d" />

4. Done! üéâ

## Local Development

Open WebUI can only import a **single Python file** per pipe. To keep the codebase maintainable, everything you actually edit lives under `src/openai_responses_manifold/`. When you are ready to share your changes, run `make build` and the tooling will run tests and regenerate the monolithic `openai_responses_manifold.py` that Open WebUI expects.

**Clone and bootstrap**

```bash
git clone https://github.com/jrkropp/open-webui-developer-toolkit.git
cd open-webui-developer-toolkit/functions/pipes/openai_responses_manifold
python -m venv .venv
source .venv/bin/activate        # Windows: .\.venv\Scripts\activate
make install-dev                 # editable install + dev extras (pytest, ruff, etc.)
python -m pip install pre-commit && pre-commit install  # optional hooks (Ruff check/format + mypy)
```

**Everyday commands**

```
make test       # run pytest
make lint       # run Ruff checks
make lint-fix   # Ruff checks with autofix
make format     # apply Ruff formatting fixes
make typecheck  # run mypy against src/
make build      # pytest + regenerate openai_responses_manifold.py for Open WebUI
make clean      # remove build artefacts and caches
```

Use `make install` when you only need runtime dependencies (e.g., smoke-testing inside Open WebUI). Otherwise stay in editable mode, make changes in `src/`, and run `make build` whenever you need a fresh single-file bundle to copy/paste or distribute. If you ever need sdists/wheels for PyPI-style distribution, invoke `python -m build` directly.

### Testing approach

The pytest suite now mixes fast unit tests with scenario-style orchestrator tests:

- `tests/fakes.py` provides reusable doubles (fake Responses client, spy event emitter, in-memory Chats store) so tests can drive the streaming loop without real network/database calls.
- Scenario coverage (`tests/test_runner_scenarios.py`) exercises ResponseRunner end-to-end: streaming completions, tool loops, and error/log flushing behavior.
- Module-focused tests (markers, request shaping, tool building, etc.) catch regressions in helpers used throughout the bundle.

Run everything via `make test`. Scenario tests rely on asyncio; pytest is already configured with `asyncio` plugin, so no extra flags are needed.

Detailed design notes and release history live under `functions/pipes/openai_responses_manifold/docs/` (`DESIGN.md`, `CHANGELOG.md`). AI handoff workpackages live in `.workpackages/`.

## Features

| Feature                            | Status          | Last updated | Notes                                                                                                                                                                                                                                                                                                        |
| ---------------------------------- | --------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Native function calling**        | ‚úÖ GA            | 2025-06-04   | Sends full JSON tool specs directly to OpenAI models that support function calling. This API-enforced method ensures reliable, validated tool calls and allows **multiple tool calls in a single response**. Much more robust than Open WebUI‚Äôs default single-call router.                                  |
| **Visible reasoning summaries**    | ‚úÖ GA            | 2025-08-07   | Enables OpenAI‚Äôs *reasoning summaries*, which explain how the model arrives at its answers. Displayed in collapsible `<details>` blocks for transparency, trust, and easier debugging.                                                                                                                       |
| **Encrypted reasoning tokens**     | ‚úÖ GA            | 2025-08-07   | Saves reasoning tokens across tool-calling ‚Äúturns‚Äù (and optionally whole conversations). Prevents the model from having to **re-reason from scratch** after each tool call, making responses faster, cheaper, and more cache-friendly.                                                                       |
| **Optimized token caching**        | ‚úÖ GA            | 2025-06-03   | Ensures all tokens‚Äîincluding hidden ones like tool calls and reasoning‚Äîare re-sent in the same order. This unlocks OpenAI‚Äôs token caching, cutting **input costs by 50‚Äì75%** and lowering latency.                                                                                                           |
| **Web search tool**                | ‚úÖ GA            | 2025-06-03   | Injects OpenAI‚Äôs `web_search` tool into supported models. With the valve enabled, the model can **self-decide when to search**. Alternatively, filters can toggle it on/off (mirroring ChatGPT‚Äôs behavior). Now emits search queries and sources via status updates for richer feedback.                                                                                                  |
| **Task model support**             | ‚úÖ GA            | 2025-08-07   | Detects when a request is for an **External Task Model** and routes it separately. Makes manifold models usable for lightweight routing or background tasks (e.g., with `gpt-4.1-nano`).                                                                                                                     |
| **Streaming responses (SSE)**      | ‚úÖ GA            | 2025-06-04   | Supports **real-time streaming**, so users can watch responses appear live as the model generates them.                                                                                                                                                                                                      |
| **Usage pass-through**             | ‚úÖ GA            | 2025-06-04   | Forwards API usage stats (tokens, caching data, etc.) into Open WebUI, visible in the frontend (hover the ‚ÑπÔ∏è icon). Gives users **transparent cost and performance insights**.                                                                                                                               |
| **Response item persistence**      | ‚úÖ GA            | 2025-06-27   | Persists hidden items (reasoning, tool calls) by embedding unique IDs in Markdown. Stored separately in the DB and reattached in later turns, so the **conversation can be rebuilt exactly** without losing hidden events.                                                                                   |
| **Open WebUI Notes compatibility** | üîÑ In progress            | 2025-07-14   | Works seamlessly with Open WebUI‚Äôs new **Notes feature** (currently preview). Ensures manifold-based models work even when chats are ephemeral (no `chat_id`).                                                                                                                                               |
| **Native status updates**         | ‚úÖ GA            | 2025-07-01   | Reports progress using Open WebUI's built-in status emitter with steps like "Thinking‚Ä¶", "Reading the question and building a plan.", "Gathering my thoughts‚Ä¶", "Exploring possible answers‚Ä¶", "Almost done‚Ä¶", and ends with "Thought for N seconds." |
| **Inline citation events**         | ‚úÖ GA (basic)    | 2025-07-28   | Adds inline citations (e.g., `[1]`) for **web search results**. Basic implementation works but still being refined. Style is adjustable with the `CITATION_STYLE` valve.                                                                                                                                     |
| **Truncation control**             | ‚úÖ GA            | 2025-06-10   | Defaults to `auto`, meaning if token limits are exceeded, older context is trimmed instead of failing. You can also set `max_tokens` via custom parameters. See OpenAI‚Äôs [Responses API docs on truncation](https://community.openai.com/t/introducing-the-responses-api/1140929/12?utm_source=chatgpt.com). |
| **Custom param pass-through**      | ‚úÖ GA            | 2025-06-14   | Supports Open WebUI‚Äôs **Custom Parameters**. Any params set in the GUI are passed through to OpenAI (e.g., `max_tokens` ‚Üí `max_output_tokens`). Lets users fine-tune behavior without editing code.                                                                                                          |
| **Regenerate ‚Üí `text.verbosity`**  | ‚úÖ GA            | 2025-08-11   | Open WebUI v0.6.19 added regenerate buttons for ‚ÄúMore Concise‚Äù / ‚ÄúAdd Details.‚Äù The manifold maps these to the `text.verbosity` parameter for GPT-5 models. Falls back to prompt injection if not supported.                                                                                                 |
| **Filter-injected tools**          | ‚úÖ GA            | 2025-08-28   | Lets developers build **companion filters** that add tools under `body.extra_tools`. The manifold merges these into `body.tools` before sending to OpenAI, removing duplicates. Enables features like **web search toggles** without breaking native function calling.                                       |
| **Image input (vision)**           | üîÑ In progress  | 2025-06-03   | Supports basic image input (Open WebUI converts uploads to base64 and forwards them). Works but inefficient for large images. A future version will switch to OpenAI‚Äôs **file upload API** for better performance.                                                                                           |
| **Image generation tool**          | üïí Backlog      | 2025-06-03   | Planned support for **creating and editing images** with OpenAI. Will include **multi-turn editing**, but depends on efficient image handling via file uploads first.                                                                                                                                        |
| **File upload / file search**      | üïí Backlog      | 2025-06-03   | Planned support for uploading files and querying their contents (e.g., PDFs, spreadsheets) directly in chat.                                                                                                                                                                                                 |
| **Code interpreter**               | üïí Backlog      | 2025-06-03   | Planned support for OpenAI‚Äôs **Python/code interpreter** tool. Would allow running code, analyzing data, and generating charts inside Open WebUI.                                                                                                                                                            |
| **Computer use**                   | üïí Backlog      | 2025-06-03   | Placeholder for OpenAI‚Äôs **computer use** tool (models interacting with apps or browsers). Not yet supported in Open WebUI.                                                                                                                                                                                  |
| **Live voice (Talk)**              | üïí Backlog      | 2025-06-03   | Planned support for **real-time voice conversations** (like ChatGPT‚Äôs Talk mode). Requires backend audio streaming support.                                                                                                                                                                                  |
| **Dynamic chat titles**            | üïí Backlog      | 2025-06-03   | Planned support for **auto-updating chat titles** during long tasks. Not yet implemented.                                                                                                                                                                                                                    |
| **MCP tool support**               | üîÑ Experimental | 2025-06-23   | Allows attaching **remote MCP servers** via the `REMOTE_MCP_SERVERS_JSON` valve. Experimental: implementation works but is not optimized, so **not production-ready**. Behavior may change.                                                                                                                  |

## Advanced Features
### Pseudo-model aliases

Use shorthand model names that automatically map to official OpenAI models with predefined reasoning levels.
Examples:

* `gpt-5-thinking` ‚Üí `gpt-5` (medium reasoning)
* `gpt-5-thinking-high` ‚Üí `gpt-5` (high reasoning)
* `o4-mini-high` ‚Üí `o4-mini` (high reasoning effort)

See table below for full list of aliases.

### Remote MCP servers (experimental)

Attach external [Model Context Protocol (MCP)](https://platform.openai.com/docs/guides/tools-remote-mcp) servers using the `REMOTE_MCP_SERVERS_JSON` valve.

* Accepts JSON describing one or more servers
  ‚ö†Ô∏è Still experimental: works, but **not recommended for production** yet.

### Filter-injected tools

Lets developers build **companion filters** that add OpenAI-style tools dynamically.

* Filters inject tools into `body.extra_tools`
* The manifold merges them into `body.tools` before sending the request
* Duplicates are removed automatically

E.g.,

```python
body.setdefault("extra_tools", []).append({
    "type": "function",
    "name": "weather_lookup",
    "description": "Get current weather by city.",
    "parameters": {
        "type": "object",
        "properties": {"city": {"type": "string"}},
        "required": ["city"],
    },
})
```

This makes features like **on-demand web search toggles** possible without breaking native function calling.

## Logging & Diagnostics

Every run buffers structured log lines and attaches them to the assistant reply as a `Logs` citation. Records flow through the in-repo `SessionLogger`, so each entry automatically includes the session id plus chat/model/message metadata and remains consistent across modules (router, HTTP client, runner, tools, persistence).

```
[INFO] [session=owui-abcd chat=chat_123 model=openai_responses.gpt-5 req=gpt-5-mini msg=msg_42] Router selected model=gpt-5 (effort=medium)
[ERROR] [session=owui-abcd chat=chat_123 model=openai_responses.gpt-5 req=gpt-5-mini msg=msg_42] invoke request failed (status=500 endpoint=https://api.openai.com/v1/responses request_id=req_123 body={"error":{"message":"invalid"}})
```

- Use `Valves.LOG_LEVEL` (pipe-level) or `UserValves.LOG_LEVEL` (per user) to control verbosity. `INHERIT` on the user valve defers to the global setting. `GLOBAL_LOG_LEVEL` continues to act as an environment override for the pipe-level valve.
- `INFO` surfaces routing decisions, HTTP status codes, tool execution milestones, and persistence warnings; `DEBUG` adds request summaries, router payloads, and masked tool arguments; `WARNING`/`ERROR` only report failures.
- Secrets (API keys, auth headers, tool tokens) are redacted automatically. Longer payloads are truncated so that the buffered citation stays readable.
- When diagnosing an issue, temporarily set `LOG_LEVEL=DEBUG`, reproduce the run, grab the `Logs` citation, then drop the valve back to `INFO`/`ERROR` once finished.

## Tested Models
The manifold should work with any model that supports the **OpenAI Responses API**.  
Below are the official model IDs that have been tested and confirmed.

### Official Model IDs

| Family            | Model ID              | Type / Modality                  | Status | Notes |
|-------------------|-----------------------|----------------------------------|:------:|-------|
| **GPT-5**         | `gpt-5`               | Reasoning                        | ‚úÖ | Standard GPT-5 reasoning model. |
|                   | `gpt-5-mini`          | Reasoning                        | ‚úÖ | Smaller, faster, lower cost than `gpt-5`. |
|                   | `gpt-5-nano`          | Reasoning                        | ‚úÖ | Ultra-lightweight reasoning; lowest cost. |
|                   | `gpt-5-chat-latest`   | Chat-tuned (non-reasoning)       | ‚úÖ | Best for polished conversation. No tool calling. ([OpenAI Platform][1]) |
| **GPT-4.1**       | `gpt-4.1`             | Non-reasoning                    | ‚úÖ | High-speed GPT-4 series model. |
| **GPT-4o**        | `gpt-4o`              | Text + image ‚Üí text              | ‚úÖ | Multimodal reasoning model. |
|                   | `chatgpt-4o-latest`   | Alias to ChatGPT‚Äôs GPT-4o build  | ‚úÖ | Matches ChatGPT‚Äôs current 4o snapshot. ([OpenAI Platform][2]) |
| **O-series**      | `o3`                  | Reasoning                        | ‚úÖ | Standard O-series reasoning model. |
|                   | `o3-pro`              | Reasoning (higher compute)       | ‚úÖ | API-only; heavier, deeper reasoning. ([OpenAI Platform][3]) |
|                   | `o3-mini`             | Reasoning                        | ‚úÖ | Lightweight O-series reasoning. ([OpenAI Platform][11]) |
|                   | `o4-mini`             | Reasoning                        | ‚úÖ | Cost-efficient O-series reasoning. ([OpenAI][12]) |
| **Deep Research** | `o3-deep-research`    | Agentic deep-research            | ‚ùå | Not yet supported. ([OpenAI Platform][4]) |
|                   | `o4-mini-deep-research` | Agentic deep-research          | ‚ùå | Not yet supported. ([OpenAI Platform][5]) |
| **Utility/Coding**| `codex-mini-latest`   | Lightweight coding / agent model | ‚úÖ | For code + lightweight reasoning. ([OpenAI Platform][6]) |

---

### Pseudo-Model Aliases (Convenience IDs)

These **aliases** are supported via the `MODELS` valve. They resolve to official models but may also apply presets (e.g., `reasoning_effort`).  
Useful for **routing, shorthand, or quick quality/cost tuning**. *(Subject to change as OpenAI updates models.)*

| Alias                                                              | Resolves To   | Preset(s)                    | Suggested Use                                                    |
| ------------------------------------------------------------------ | ------------- | ---------------------------- | ---------------------------------------------------------------- |
| `gpt-5-auto`                                                       | Dynamic GPT-5 | ‚Äî                            | Auto-routes between GPT-5 chat/mini/nano.                        |
| `gpt-5-thinking`                                                   | `gpt-5`       | Medium reasoning             | General high-quality tasks. (\[OpenAI]\[8])                      |
| `gpt-5-thinking-minimal`                                           | `gpt-5`       | `reasoning_effort="minimal"` | Faster/cheaper reasoning. (\[OpenAI]\[8])                        |
| `gpt-5-thinking-high`                                              | `gpt-5`       | `reasoning_effort="high"`    | Hard problems; maximum quality. (\[OpenAI]\[8])                  |
| `gpt-5-thinking-mini`                                              | `gpt-5-mini`  | Medium reasoning             | Budget-optimized reasoning. (\[OpenAI Platform]\[9])             |
| `gpt-5-thinking-mini-minimal`                                      | `gpt-5-mini`  | `reasoning_effort="minimal"` | Lowest-cost reasoning on mini. (\[OpenAI Platform]\[9])          |
| `gpt-5-thinking-mini-high`                                         | `gpt-5-mini`  | `reasoning_effort="high"`    | Higher-effort reasoning on mini. (\[OpenAI Platform]\[9])        |
| `gpt-5-thinking-nano`                                              | `gpt-5-nano`  | Medium reasoning             | Ultra-cheap reasoning; routing/triage. (\[OpenAI Platform]\[10]) |
| `gpt-5-thinking-nano-minimal`                                      | `gpt-5-nano`  | `reasoning_effort="minimal"` | Absolute cheapest reasoning. (\[OpenAI Platform]\[10])           |
| `gpt-5-thinking-nano-high`                                         | `gpt-5-nano`  | `reasoning_effort="high"`    | High-effort reasoning at lowest tier. (\[OpenAI Platform]\[10])  |
| `o3-mini-high`                                                     | `o3-mini`     | `reasoning_effort="high"`    | Small + fast, deeper reasoning. (\[OpenAI Platform]\[11])        |
| `o4-mini-high`                                                     | `o4-mini`     | `reasoning_effort="high"`    | Balanced cost + deeper reasoning. (\[OpenAI]\[12])               |
| *(reserved)* `gpt-5-main`, `gpt-5-main-mini`, `gpt-5-thinking-pro` | ‚Äî             | ‚Äî                            | Reserved placeholders; no current API models. (\[OpenAI]\[8])    |

---

[1]: https://platform.openai.com/docs/models/gpt-5-chat-latest?utm_source=chatgpt.com "Model - OpenAI API"  
[2]: https://platform.openai.com/docs/models/chatgpt-4o-latest?utm_source=chatgpt.com "Model - OpenAI API"  
[3]: https://platform.openai.com/docs/models/o3-pro?utm_source=chatgpt.com "Model - OpenAI API"  
[4]: https://platform.openai.com/docs/models/o3-deep-research?utm_source=chatgpt.com "o3-deep-research"  
[5]: https://platform.openai.com/docs/guides/deep-research?utm_source=chatgpt.com "Deep research - OpenAI API"  
[6]: https://platform.openai.com/docs/models/codex-mini-latest?utm_source=chatgpt.com "Codex mini"  
[7]: https://platform.openai.com/docs/models/gpt-5?utm_source=chatgpt.com "Model - OpenAI API"  
[8]: https://openai.com/index/introducing-gpt-5-for-developers/?utm_source=chatgpt.com "Introducing GPT-5 for developers"  
[9]: https://platform.openai.com/docs/models/gpt-5-mini?utm_source=chatgpt.com "GPT-5 mini"  
[10]: https://platform.openai.com/docs/models/gpt-5-nano?utm_source=chatgpt.com "Model - OpenAI API"  
[11]: https://platform.openai.com/docs/models/o3-mini?utm_source=chatgpt.com "Model - OpenAI API"  
[12]: https://openai.com/index/introducing-o3-and-o4-mini/?utm_source=chatgpt.com "Introducing OpenAI o3 and o4-mini"  


## GPT-5 Model Support

The Responses Manifold supports the full **GPT-5 family** of models currently exposed in the API:

- `gpt-5` *(reasoning model)*
- `gpt-5-mini` *(reasoning model)*
- `gpt-5-nano` *(reasoning model)*
- `gpt-5-chat-latest` *(non-reasoning, fine-tuned for chat, does NOT support function calling / tools)*

In the public **ChatGPT app**, choosing *GPT-5* doesn‚Äôt mean you‚Äôre using one fixed model. Behind the scenes, OpenAI runs a **router layer** that inspects your request and decides whether to send it to a reasoning, minimal-reasoning, or non-reasoning GPT-5 variant.

This router is **not available in the API**. When using the API, you must select the model explicitly (`gpt-5`, `gpt-5-mini`, `gpt-5-nano`, or `gpt-5-chat-latest`).

To bridge this gap, the manifold includes an experimental **`gpt-5-auto`** model.  
- It isn‚Äôt a real OpenAI model ID.  
- Instead, the request is first passed to a lightweight classifier (currently `gpt-5-nano` with minimal reasoning + a routing prompt).  
- The classifier chooses the most suitable GPT-5 endpoint (reasoning, minimal-reasoning, or chat) for your request.  

‚ö†Ô∏è This router is an **early proof of concept** and will be optimized further in future releases.

### What you need to know
1. **Reasoning vs. non-reasoning**  
   - `gpt-5`, `gpt-5-mini`, and `gpt-5-nano` are **reasoning models** by default.  
   - Setting `reasoning_effort="minimal"` makes them cheaper/faster but they still perform reasoning.  
   - For a true **non-reasoning chat model**, use `gpt-5-chat-latest`.

2. **Tool calling**  
   - Reasoning models support **native tool calling** (function calls, web search, etc.).  
   - `gpt-5-chat-latest` does **not** support tool calling ‚Äî it‚Äôs tuned only for polished chat.  
   - A future `gpt-5-main` model may bring **non-reasoning + tool support**.

3. **Latency and performance**  
   - Even with `"minimal"` effort, reasoning models take time to ‚Äúthink.‚Äù  
   - For **ultra-low latency tasks**, use `gpt-4.1-nano` until a GPT-5 task model is released.

4. **Output style**  
   - `gpt-5-chat-latest` is tuned for smooth, conversational answers and usually works without a system prompt.  
   - Reasoning models work best with a short style prompt (e.g., ‚ÄúConcise Markdown with headings and lists‚Äù).  
   - Example prompts are provided in the `system_prompts` folder.


[1]: https://openai.com/index/introducing-gpt-5-for-developers/ "Introducing GPT-5 for developers | OpenAI"  
[2]: https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf "GPT-5 System Card (Aug 7, 2025)"


## How It Works (Design Notes)

### Persisting non-message items (function calls, tool outputs, reasoning tokens, ‚Ä¶)

The **OpenAI Responses API** doesn‚Äôt just return text. A single response can include:
- reasoning steps,
- function/tool calls,
- tool results,
- assistant messages.

By default, **Open WebUI only saves** the `role` (`user` / `assistant`) and the assistant‚Äôs **visible text content**.  
That means all the ‚Äúhidden work‚Äù (reasoning, tools used, results returned) would normally disappear.  
If we don‚Äôt capture it:
- tool calls may run again on regenerate,
- reasoning tokens are wasted (higher cost, slower),
- the model‚Äôs exact sequence of actions is lost.

The manifold solves this by persisting **all items** in sequence, not just the visible text.

---

### The persistence challenge

Open WebUI‚Äôs conversation history is built around a simple `messages[]` array where each entry looks like:

```json
{ "role": "user" | "assistant", "content": "..." }
````

* `role` identifies who sent the message.
* `content` is always rendered in the chat UI as visible text.

Here‚Äôs the problem:

* **Upstream filters** (which inject context, rewrite prompts, toggle tools, etc.) only modify the `messages[]` array.
* If the manifold built its own parallel storage, it would ignore those filter changes ‚Äî breaking compatibility and leaving the manifold out of sync.

So we must keep using the **same `messages[]` array** that Open WebUI and its filters rely on.
But since `content` is always displayed in the UI, we need a way to tuck hidden data into assistant messages **without showing it to the user**.

---

### The solution: invisible markers

The manifold injects **empty Markdown reference links** into the assistant‚Äôs response text.
These links are ignored by the Open WebUI frontend (they don‚Äôt render), but they carry stable IDs that point to the hidden items stored in the DB.

Example marker:

```
[openai_responses:v2:function_call:01HX4Y2VW5VR2Z2H]: #
```

**Marker format:**

```
[openai_responses:v2:<item_type>:<id>[?model=<model_id>&k=v...]]: #
```

* `<item_type>` = event type (e.g., `function_call`, `reasoning`)
* `<id>` = unique 16-character ID
* optional query params (e.g., `model`)

> **Why not embed JSON directly?**
> Markers keep assistant messages lightweight and clipboard-safe, while the full payloads remain in the DB.

---

### Example: function call flow

1. **User asks a question**

```json
{ "role": "user", "content": "Calculate 34234 multiplied by pi." }
```

2. **Model emits a tool call**

```json
{
  "type": "function_call",
  "name": "calculator",
  "arguments": "{\"expression\":\"34234*pi\"}",
  "status": "completed"
}
```

* Stored under ID `01HX4Y2VW5VR2Z2H`
* Marker injected into assistant output:

```
[openai_responses:v2:function_call:01HX4Y2VW5VR2Z2H]: #
```

3. **Tool result is persisted the same way**

```
[openai_responses:v2:function_call_output:01HX4Y2VW6B091XE]: #
```

4. **Assistant shows visible text**

```
34234 multiplied by œÄ ‚âà 107,549.28.
```

5. **Final stream = hidden markers + visible text**

```
[openai_responses:v2:function_call:01HX4Y2VW5VR2Z2H?model=openai_responses.gpt-4o]: #
[openai_responses:v2:function_call_output:01HX4Y2VW6B091XE?model=openai_responses.gpt-4o]: #
The result of \(34234 \times \pi\) is approximately 107,549.28.
```

---

### Why this matters

By combining hidden markers with DB persistence:

* **No duplicate work** ‚Üí tool calls and reasoning aren‚Äôt re-run on regenerate
* **Lower cost & latency** ‚Üí caching saves \~50‚Äì75% input tokens
* **Filter compatibility** ‚Üí upstream filters can still modify `messages[]` normally
* **Full fidelity history** ‚Üí the exact reasoning + tool sequence is preserved and replayable

---

> **Tip for debugging:** Open browser **DevTools ‚Üí Network**, inspect the chat POST payload, and you‚Äôll see the hidden markers alongside the visible messages.
